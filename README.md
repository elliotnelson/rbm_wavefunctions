# RBM Learning of Multi-Modal Distributions

This repository contains a TensorFlow implementation of Restricted Boltzmann Machine (RBM) learning, adapted from [github.com/MelkoCollective/ICTP-SAIFR-MLPhysics/tree/master/RBM_CDL](https://github.com/MelkoCollective/ICTP-SAIFR-MLPhysics/tree/master/RBM_CDL).

The current goal is to train the RBM to efficiently represent a simple simple multi-modal data distribution (see below for the motivation).  In particular, I am exploring the success of different cost functions at maintaining probability mass over multiple modes while also suppressing probability mass between modes, where probability can be extremely low.

In general, unsupervised data are drawn from some unknown probability distribution, and the cross-entropy or KL divergence between the true distribution *p(x)* and model distribution <a href="https://www.codecogs.com/eqnedit.php?latex=p_M(x)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_M(x)" title="p_M(x)" /></a> must be approximated as a sum over the data samples available:

<a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{L}&space;=&space;\sum_x&space;p(x)\log&space;p_M(x)&space;\approx&space;\frac{1}{N}\sum_i^N&space;\log&space;p_M(x_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{L}&space;=&space;\sum_x&space;p(x)\log&space;p_M(x)&space;\approx&space;\frac{1}{N}\sum_i^N&space;\log&space;p_M(x_i)" title="\mathcal{L} = \sum_x p(x)\log p_M(x) \approx \frac{1}{N}\sum_i^N \log p_M(x_i)" /></a>

However, if we are able to compute the true probability *p(x)* for a sample configuration *x* (say, if we are interested in using an RBM to learn a particular distribution *p(x)* obtained from some quantum system of interest), we can use importance sampling and train on samples <a href="https://www.codecogs.com/eqnedit.php?latex=x_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_j" title="x_j" /></a> drawn from a simpler distribution *q(x)*, weighting each by *p(x)/q(x)* when computing the cross-entropy:

<a href="https://www.codecogs.com/eqnedit.php?latex=\mathcal{L}&space;\approx&space;\frac{1}{N}\sum_j^N&space;\frac{p(x_j)}{q(x_j)}&space;\log&space;p_M(x_j)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathcal{L}&space;\approx&space;\frac{1}{N}\sum_j^N&space;\frac{p(x_j)}{q(x_j)}&space;\log&space;p_M(x_j)" title="\mathcal{L} \approx \frac{1}{N}\sum_j^N \frac{p(x_j)}{q(x_j)} \log p_M(x_j)" /></a>

We can also use other cost functions which do not integrate over the true distribution. In particular, the reverse KL divergence,

<a href="https://www.codecogs.com/eqnedit.php?latex=D_{KL}(p_M||p)&space;=&space;-&space;\sum_x&space;p_M(x)\log\left(\frac{p(x)}{p_M(x)}\right)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{KL}(p_M||p)&space;=&space;-&space;\sum_x&space;p_M(x)\log\left(\frac{p(x)}{p_M(x)}\right)" title="D_{KL}(p_M||p) = - \sum_x p_M(x)\log\left(\frac{p(x)}{p_M(x)}\right)" /></a>

unlike the usual cross-entropy or KL divergence, <a href="http://www.codecogs.com/eqnedit.php?latex=D_{KL}(p||p_M)" target="_blank"><img src="http://latex.codecogs.com/gif.latex?D_{KL}(p||p_M)" title="D_{KL}(p||p_M)" /></a>, integrates over the (RBM) model distribution *p_M*. If we are able to compute the true probability *p(x)* for each sample *x* , then we can approximate the reverse KL divergence with samples from *p(x)*:

<a href="https://www.codecogs.com/eqnedit.php?latex=D_{KL}(p_M||p)&space;\approx&space;-&space;\frac{1}{N}&space;\sum_j^N&space;\frac{p_M(x_j)}{p(x_j)}\log\left(\frac{p(x_j)}{p_M(x_j)}\right)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D_{KL}(p_M||p)&space;\approx&space;-&space;\frac{1}{N}&space;\sum_j^N&space;\frac{p_M(x_j)}{p(x_j)}\log\left(\frac{p(x_j)}{p_M(x_j)}\right)" title="D_{KL}(p_M||p) \approx - \frac{1}{N} \sum_j^N \frac{p_M(x_j)}{p(x_j)}\log\left(\frac{p(x_j)}{p_M(x_j)}\right)" /></a>

The reverse KL divergence penalizes the model for failing to assign low probability *p_M(x)* to unlikely configurations with low *p(x)*. Using an importance sampling distribution *q(x)* to include such configurations, along with a cost function which penalizes in this way, we can more easily train the RBM to suppress probability mass *p_M(x)* between the modes of *p(x)*.  On the other hand, the reverse KL divergence has a tendency to lock onto a single mode, and struggles to maintain probability mass over all modes as well as the cross-entropy.  (Cf. Goodfellow, ["NIPS 2016 Tutorial:
Generative Adversarial Networks"](https://arxiv.org/pdf/1701.00160.pdf).)  I am currently looking for a good way to resolve this, in particular by looking for conditions such that the model behaves well when perturbed around the solution *p_M(x)=p(x)*.

This is early-stage work in progress with collaborators, and the goal is to extend results for multi-modal probability distributions to entangled quantum wavefunctions with a similar structure.

Simulations of quantum systems (on classical computers) require computational resources that are exponential in the size of the system. However, most quantum systems exhibit classical behavior on large (e.g. macroscopic) scales, and isolating the variables which behave classically can allow for a speedup in simulation of that part of the system.  These "feature" variables are encoded in the entanglement structure of the wavefunction in the form of redundant information, with different modes of the wavefunction corresponding to different values of these classical variables.  If a generative model like an RBM can efficiently encode this multi-modal structure (e.g. with a polynomial number of hidden configurations, compared to an exponential number of visible configurations), then the classically relevant part of a quantum system could be identified by training the model to match the true wavefunction.
